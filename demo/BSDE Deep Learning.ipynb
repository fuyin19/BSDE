{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{PDut}{\\frac{\\partial u}{\\partial t}}\n",
    "\\newcommand{PDuS}{\\frac{\\partial u}{\\partial S}}\n",
    "\\newcommand{PDuSS}{\\frac{\\partial ^2u}{\\partial S^2}}\n",
    "\\newcommand{eps}{\\varepsilon}\n",
    "$$\n",
    "$\\newcommand{\\a}{\\alpha} \\newcommand{\\s}{\\sigma} \\newcommand{\\half}{\\frac{1}{2}} \\newcommand{\\F}{\\mathcal{F}} \\newcommand{\\P}{\\mathbb{P}} \\newcommand{\\par}{\\partial} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\argmin}{\\arg\\!\\min} \\newcommand{\\E}{\\mathbb E} \\newcommand{\\lb}{\\left [} \\newcommand{\\rb}{\\right ]} \\newcommand{\\U}{\\mathcal{U}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deep learning based algorithm for BSDE\n",
    "In this section, we will explore the implementation of a BSDE solver using deep learning structures. The algorithms are selected from [this paper](https://arxiv.org/pdf/1902.01599.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBDP1\n",
    "\n",
    "The non-linear Feynman-Kac formula is stated as the following,\n",
    "\n",
    "Given $t\\in[0, T]$, \n",
    "* $(\\Omega, \\mathcal{F}, \\{\\F_t\\}_{0\\leq t \\leq T}, \\mathbb{P})$, the filtered probability space\n",
    "* $\\{X_s\\}_{t\\leq s \\leq T}$ satisfies the forward SDE\n",
    "\n",
    "$$\n",
    "    dX_s = \\mu(s,X_s) ds + \\s(s,X_s) dW_s, \\quad X_t = x.\n",
    "$$\n",
    "* $\\{Y_s\\}_{t\\leq s \\leq T}$ satisfies the backward SDE\n",
    "\n",
    "$$\n",
    "    dY_s =-f(s, X_s, Y_s, Z_s)ds + Z_sdW_s, \\quad Y_T = g(X_T) \\tag{1.1.1}\n",
    "$$\n",
    "\n",
    "* $(\\mu, \\s)$ and $(f, g)$ are the determinstic functions that satisfies the Lipschiz conditions and growth constraints\n",
    "\n",
    "Suppose the (unique) function $u:[0, T] \\to \\R$ satisfies the following nonlinear PDE\n",
    "\n",
    "$$  \n",
    "\\begin{align*}\n",
    "   & \\par_tu + \\mathcal{L}u + f(t, x, u,  D_xu \\s(t,x)) = 0 \\tag{2.2.4} \\\\\n",
    "   & u(T, x) = g(x) \\tag{2.2.5}\\\\ \n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "then the unique pair of $\\F_t$-adapted process $(Y_s, Z_s)_{t \\leq s \\leq T}$ \n",
    "\n",
    "\\begin{align*}\n",
    "    Y_s &:= u(s, X_s)  \\\\\n",
    "    Z_s &:= D_xu(s, X_s) \\s(s,X_s)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "is the solution to BSDE $(1.1.1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### highlight of the algorithm\n",
    "Suppose we have the same discretization of the FBSDE as in section 3.1. Now, recall that\n",
    "\\begin{align*}\n",
    "    Y^P_{n} - Y^P_{n-1} &= -f(t_{n-1}, X^P_{n-1}, Y^P_{n-1},  Z^P_{n-1})\\Delta t + Z^P_{n-1} \\Delta W_{n} \\tag{3.1.4} \\\\\n",
    "\\end{align*}\n",
    "this means\n",
    "\n",
    "\\begin{align*}\n",
    "    Y^P_{n} &= Y^P_{n-1} -f(t_{n-1}, X^P_{n-1}, Y^P_{n-1},  Z^P_{n-1})\\Delta t + Z^P_{n-1} \\Delta W_{n} \\\\\n",
    "    &:= F(t_n, X^P_{n-1}, Y^P_{n-1}, Z^P_{n-1}, \\Delta t, \\Delta W_{n}) \\tag{4.1.1}\n",
    "\\end{align*}\n",
    "\n",
    "Now, consider a sequence of neural network approximation for $(Y^P_n, Z^P_n)_{n=0}^{N-1}$ as the following\n",
    "\n",
    "\\begin{align*}\n",
    "    Y^P_{n} &= u(t_n, X^P_n) &\\sim \\mathscr{U}_{n}(X^P_n ; \\zeta_n) \\\\\n",
    "    Z^P_{n} &= D_xu(s, X_s) \\s(s,X_s) &\\sim \\mathcal{Z}_{n}(X^P_n; \\eta_n)\n",
    "\\end{align*}\n",
    "\n",
    "where we define $\\theta_n:=(\\zeta_n, \\eta_n)$ to be the set of parameters of the pair of neural network approximation $(\\mathscr U_n, \\mathcal Z_n)$ at time $t_n$.\n",
    "\n",
    "\n",
    "Then, the equation $(4.1.1)$ becomes\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathscr{U}_n(t_n, X^P_n) &= F(t_n, X^P_{n-1}, \\mathscr U_{n-1}, \\mathcal Z_{n-1}, \\Delta t, \\Delta W_{n}) \\tag{4.1.2}\n",
    "\\end{align*}\n",
    "\n",
    "Equation $(4.1.2)$ is the key to the algorithm for fitting the above neutral network approximation iteratively backward, starting from $t_N=T$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### backward iteration\n",
    "\n",
    "Here is the proposed scheme of DBDP1. We are aim to derive the optimal estimation $(\\hat{\\mathscr U}_n, \\hat{\\mathcal Z}_n)_{n=1}^N$ with the parameter $\\{\\theta^*_n\\}_{n=1}^N = (\\zeta^*_n, \\eta^*_n)_{n=1}^N$ given the discretization scheme $P$.\n",
    "* For $n = N$, define $\\hat{\\mathscr U}_N := g$.\n",
    "* For $n = N-1,...,0$, we solve the optimization problem,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\theta_n^* &= \\argmin_{\\theta} L_n(\\theta) \\\\\n",
    "    & = \\argmin_{\\theta} \\frac{1}{M} \\sum_{m=1}^M \\left( \\hat{\\mathscr U}_n(X^P_{m, n}) - F(t_n, X^P_{m, n-1}, \\mathscr U_{n-1} (\\zeta), \\mathcal Z_{n-1} (\\eta), \\Delta t, \\Delta W_n) \\right)^2\n",
    "\\end{align*}\n",
    "\n",
    "Then, we store the optimal parameters $\\theta^*_n$ for the model\n",
    "\n",
    "$$\n",
    "    \\hat{\\mathscr U}_n = \\mathscr U_n(\\cdot, \\zeta^*_n) \\quad \\text{and} \\quad \\hat{\\mathcal Z}_n = \\mathcal Z_n(\\cdot, \\eta^*_n)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we immediately notice that the LHS of equation $(4.1.2)$ is known if we solve the following optimization  problem $(4.1.3)$ iteratively backward from $t_N=T$, where we are given $\\mathscr U_N(T, X_T) := Y_T = g(X_T)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
