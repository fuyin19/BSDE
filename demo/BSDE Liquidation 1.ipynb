{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/finn/Desktop/Capstone-BSDE/files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 19:52:06.288707: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow version 2.11.0\n",
      "WARNING:tensorflow:From /Users/finn/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from bsde.config import ConfigHJB1, ConfigLSMC, ConfigDeepBSDE, ConfigFBSNN\n",
    "from bsde.solver.lsmc import LSMCLinear\n",
    "from bsde.solver.deep_bsde import DeepBSDESolver\n",
    "from bsde.dynamics.liquidation1 import HJB_liquidation1_FBSDE, HJB_liquidation1_solver\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algo & HF trading - P140\n",
    "\n",
    "$\\newcommand{\\a}{\\alpha} \\newcommand{\\s}{\\sigma} \\newcommand{\\half}{\\frac{1}{2}} \\newcommand{\\F}{\\mathcal{F}} \\newcommand{\\P}{\\mathbb{P}} \\newcommand{\\par}{\\partial} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\argmin}{\\arg\\!\\min} \\newcommand{\\E}{\\mathbb E} \\newcommand{\\lb}{\\left [} \\newcommand{\\rb}{\\right ]} \\newcommand{\\U}{\\mathcal{U}}$ $\\newcommand{\\lm}{\\begin{pmatrix}} \\newcommand{\\rm}{\\end{pmatrix}} \\newcommand{\\eps}{\\varepsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model follows\n",
    "\n",
    "\\begin{split}\n",
    "dS_t &= -g(\\nu_t) dt + \\sigma_s dW^t \\\\\n",
    "dQ_t & = -\\nu_t dt \\\\\n",
    "\\hat S_t & = S_t - \\left(\\half \\Delta + f(\\nu_t) \\right)\n",
    "\\end{split}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value function is, under the assumption that $\\Delta = 0$ and $f(v) = k v$, \n",
    "$$ H(t,s,q) = \\sup_{\\nu_t \\in \\mathcal A(t,T)} E_t\\left( \\int_t^T \\hat S_u \\nu_u du \\right) = \\sup_{\\nu_t \\in \\mathcal A(t,T)} E_t\\left( \\int_t^T (S_u - k \\nu_u) \\nu_u du \\right) $$\n",
    "\n",
    "which is solution to:\n",
    "$$ \\partial_t H + \\half \\sigma_S^2 \\partial_{ss}H + \\sup_v {(s-kv)v - v\\partial_q H} = 0 $$\n",
    "\n",
    "The supremum is achieved for $v^* = \\frac{1}{2k}(s-\\partial_q H)$ and yields\n",
    "$$ \\partial_t H + \\half \\sigma_S^2 \\partial_{ss}H + \\frac{1}{4k}(s-\\partial_q H)^2 = 0 $$\n",
    "\n",
    "In order to ensure that $Q_T = 0$, the value funtion must satisfy $\\lim_{t \\uparrow T} H(t,s,q) = -\\infty$ for $q \\neq 0$ and $H(T,s,0) = 0$.\n",
    "\n",
    "This leads to \n",
    "$$ H(t,s,q) = qs - q^2 \\frac{k}{T-t} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical consideration purposes, we need to remove the constraints that $\\lim_{t \\uparrow T} H(t,s,q) = -\\infty$ which can be done by setting $H(T,s,q) = qs -\\lambda q^2$ for $\\lambda >0,(\\lambda>>0)$. This similar to penalizing the remaining inventory at maturity. The analytical solution becomes\n",
    "$$ H_{\\lambda}(t,s,q) = qs - q^2 \\left(\\frac{1}{\\lambda} + \\frac{1}{k}(T-t)\\right)^{-1} $$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another numerical trick (that needs to be introduced to deal with the singularity of the PDE) is to consider the solution $H_{\\lambda, \\eps}$ to the PDE\n",
    "$$ \\partial_t H + \\half \\sigma_S^2 \\partial_{ss}H + \\half \\eps^2 q^2 \\partial_{qq}H  + \\frac{1}{4k}(s-\\partial_q H)^2 = 0 $$\n",
    "\n",
    "We now want to cast this PDE with it's corresponding BSDE $Y_t = H(t,X_t) = H(t,S_t, Q_t)$ process.\n",
    "\n",
    "Noting $X_t = (S_t, Q_t)^{T}$ and $D_x H = (\\partial_s H, \\partial_q H)^T$, we consider the SDEs:\n",
    "\\begin{split}\n",
    "dS_t &= \\sigma_s dW^S_t \\\\\n",
    "dQ_t &= \\eps Q_t dW^Q_t \\\\\n",
    "\\end{split}\n",
    "\n",
    "with $\\langle W^S, W^Q \\rangle_t = 0$, the volatility matrix is\n",
    "$$ \\s_t= \\lm \\s_s & 0 \\\\ 0 & \\eps  Q_t\\rm $$\n",
    "\n",
    "So that \n",
    "$$ Z_t = \\s_t^T D_x H(t,X_t) = \\lm \\s_s \\partial_s H(t,X_t) & 0 \\\\ 0 & \\eps  Q_t \\partial_q H(t,X_t)\\rm $$\n",
    "\n",
    "We now see that we need to set\n",
    "$$ f(s,q, Z) = \\frac{1}{4k}(a \\cdot X + A \\cdot Z)^2 = \\frac{1}{4k}(s-\\partial_q H)^2 $$\n",
    "\n",
    "with \n",
    "\\begin{split}\n",
    "a &= (1,0)^T \\\\\n",
    "A &= \\left(0, -\\frac{1}{\\eps q}\\right)^T \\\\\n",
    "\\end{split}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Remark:</b> We could have chosen $dQ_t = \\eps dW^Q_t$, but the issue is that the term $(s-\\partial_qH)^2$ is in $q^2$ and the term in $\\partial_{qq}H$ is a non-zero constant that cannot offset $(s-\\partial_qH)^2$. Choosing $dQ_t = \\eps Q_t dW^Q_t$ instead is useful to get an analytical expression of $H(t,s,q)$.\n",
    "    \n",
    "We could still proceed with the guess $H(t,s,q) = a(t) + qs + q^2 h(t)$, and assuming a PDE constant with $dQ_t = \\eps dW^Q_t$, we would get\n",
    "$$ \\partial_t H + \\half \\sigma_s^2 \\partial_{ss}H + \\half \\eps^2 \\partial_{qq} H + \\frac{1}{4k}(s-\\partial_q H)^2 = a'(t) + q^2 h'(t) + \\eps^2 h(t) + \\frac{1}{k}q^2 h(t)^2  $$\n",
    "    \n",
    "Then we need to choose $a(t)$ and $h(t)$ such that $h'(t) + \\frac{1}{k}h^2(t) = 0$ and $a'(t)= h(t)$ which is now trivial and yields for $h(T) \\neq 0$:\n",
    "$$ h(t) = \\left( \\frac{1}{h(T)} - \\frac{1}{k}(T-t) \\right)^{-1}$$\n",
    "$$ a(t) = a(T) + \\frac{1}{k} \\ln \\left(\\frac{h(T) - \\frac{1}{k}(T-t)}{h(T)} \\right) $$\n",
    "    \n",
    "If $h(T) = 0$, then $a(t) = \\frac{1}{k} \\ln(T-t)$ leads to a choice that unfortunately always gives $\\lim_{t \\uparrow T} H(t,s,q) = -\\infty$ even when $q=0$.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Remark:</b> A possible extension, more sophisticated but that could be exact is to write\n",
    "\\begin{split}  \n",
    " 0 &=  \\partial_t H + \\half \\sigma_S^2 \\partial_{ss}H + \\frac{1}{4k}(s-\\partial_q H)^2 \\\\\n",
    "&= \\partial_t H + \\half \\sigma_S^2 \\partial_{ss}H + \\half \\eps^2 q^2 \\partial_{qq}H + \\frac{1}{4k}(s-\\partial_q H)^2 \\\\\n",
    "& = \\partial_t H + \\mathcal L H + f(t, s,q, H, D_xH, D_x^2 H)\n",
    "\\end{split}\n",
    "    \n",
    "is a fully-non linear PDE with a 2D-BSDE reprensetation\n",
    "\\begin{split}  \n",
    " dY_t &=  -f(t,Y_t, X_t, Z_t, \\Gamma_t, A_t) + Z^T_t dW_t \\\\\n",
    "dZ_t &= A_t dt + \\Gamma_t dW_t \\\\\n",
    "\\end{split}    \n",
    " \n",
    "Setting $Y_t = u(t,X_t)$, assuming enough regularity and applying Ito's formula yields $Z_t = D_x u$, $\\Gamma_t = D^2_x u$ and $A_t = \\mathcal L u$.    \n",
    "    \n",
    "A standard LSMC simulation of such equation is similar to the Markov FBSDE, but with one more layer of regression:\n",
    "* $Y_T = g(X_T)$\n",
    "* $Z^T_t$ is still obtained from $ Z^T_t = \\frac{d}{dt}\\langle Y,W \\rangle_t = E_t(dY_t W_t^T) $\n",
    "* $\\Gamma_t = \\frac{d}{dt}\\langle Z,W \\rangle_t = E_t(dZ_t W_t^T)$\n",
    "* $A_t = E_t(dZ_t / dt)$ is regressed on a basis of functions.\n",
    "* $Y_{t_n}$ comes from $Y_{t_{n+1}}$ and the regression $E_t(dY_t / dt) = -f(t,Y_t, X_t, Z_t, \\Gamma_t, A_t)$  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical Y0: -98406.37450199202\n"
     ]
    }
   ],
   "source": [
    "# Simulation parameters\n",
    "T = 0.25\n",
    "dt = 1 / 252\n",
    "N = int(T/dt)\n",
    "d = 2\n",
    "d1 = 2\n",
    "d2 = 1\n",
    "seed = 42\n",
    "\n",
    "# Model parameters\n",
    "x0 = np.array([30., 10000.])   # S_0, q_0\n",
    "epsilon = 0.001\n",
    "sig_s = 0.5\n",
    "lb = 1.\n",
    "k = 0.001\n",
    "\n",
    "# config of the pde\n",
    "cfg_HJB1 = ConfigHJB1(sig_s=sig_s, eps=epsilon, lb=lb, k=k, T=T, d=d, d1=d1, d2=d2)\n",
    "\n",
    "print('Analytical Y0: {}'.format(x0[0]*x0[1] - x0[1]**2 / (1/lb + T/k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Y_0 = H_{\\lambda}(0,s,q) = qs - q^2 \\left(\\frac{1}{\\lambda} + \\frac{T}{k}\\right)^{-1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 19:52:11.642924: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-09 19:52:23.271876: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It: 0, Loss: 7.956e+16, Y0: -6.659, Time: 6.13, Learning Rate: 7.000e-03\n",
      "It: 10, Loss: 7.952e+16, Y0: 10.813, Time: 0.39, Learning Rate: 7.000e-03\n",
      "It: 20, Loss: 7.950e+16, Y0: -1.898, Time: 0.39, Learning Rate: 7.000e-03\n",
      "It: 0, Loss: 7.960e+16, Y0: -2.321, Time: 0.04, Learning Rate: 2.000e-03\n",
      "It: 10, Loss: 7.953e+16, Y0: -8.421, Time: 0.40, Learning Rate: 2.000e-03\n",
      "It: 20, Loss: 7.949e+16, Y0: -3.348, Time: 0.44, Learning Rate: 2.000e-03\n",
      "It: 30, Loss: 7.949e+16, Y0: -15.555, Time: 0.41, Learning Rate: 2.000e-03\n",
      "It: 0, Loss: 7.957e+16, Y0: -5.283, Time: 0.04, Learning Rate: 1.000e-04\n",
      "It: 10, Loss: 7.946e+16, Y0: -7.375, Time: 0.41, Learning Rate: 1.000e-04\n",
      "It: 20, Loss: 7.946e+16, Y0: -8.045, Time: 0.41, Learning Rate: 1.000e-04\n"
     ]
    }
   ],
   "source": [
    "# FBSNN -- Maziar Raissi (current method)\n",
    "if True:\n",
    "    tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "    # Batch size\n",
    "    batch_size = 8\n",
    "\n",
    "    # Config of the solver\n",
    "    cfg_FBSNN = ConfigFBSNN(x0=x0.reshape(1, d), N=N, M=batch_size, dt=dt, seed=seed, layers=[d+1] + 4*[256] + [1])\n",
    "\n",
    "    # Train\n",
    "    HJB_FBSNN = HJB_liquidation1_solver(config_dynamic=cfg_HJB1, config_solver=cfg_FBSNN)\n",
    "    HJB_FBSNN.train(N_Iter=3 * 10 ** 1, learning_rate=7e-3)  # change the N_Iter and learning rate\n",
    "    HJB_FBSNN.train(N_Iter=4 * 10 ** 1, learning_rate=2e-3)  # change the N_Iter and learning rate\n",
    "    HJB_FBSNN.train(N_Iter=3 * 10 ** 1, learning_rate=1e-4)  # change the N_Iter and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep BSDE -- founding paper (will not be considered currrently)\n",
    "\n",
    "if False:\n",
    "    tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "    # Batch size\n",
    "    batch_size = 64\n",
    "\n",
    "    # Config of the solver\n",
    "    cfg_deep_solver = ConfigDeepBSDE(N=N, dt=dt, seed=42, x0=x0,\n",
    "                                     y_init_range=[-98000, -98100],\n",
    "                                     n_hiddens=[10+d, 10+d],\n",
    "                                     lr_values=[2e-3, 1e-3],\n",
    "                                     lr_boundaries=[6000],  # change this if needed\n",
    "                                     n_iterations=1000,     # change this if needed\n",
    "                                     batch_size=batch_size,\n",
    "                                     valid_size=64,\n",
    "                                     report_freq=100,\n",
    "                                     dtype='float64',\n",
    "                                     verbose=True)\n",
    "\n",
    "    # Train\n",
    "    FBSDE = HJB_liquidation1_FBSDE(config=cfg_HJB1, exclude_spot=True)\n",
    "    deep_solver = DeepBSDESolver(FBSDE, cfg_deep_solver)\n",
    "    deep_solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignore the part below -- Just for testing the BSDE implementation in FBSNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to enable v2 behavior of tensorflow\n",
    "# Simulation parameters\n",
    "M = 2 ** 12\n",
    "T = 0.25\n",
    "dt = 1 / 252\n",
    "N = int(T/dt)\n",
    "d = 2\n",
    "d1 = 2\n",
    "d2 = 1\n",
    "seed = 42\n",
    "\n",
    "# Model parameters\n",
    "x0 = np.array([30., 10000.])   # S_0, q_0\n",
    "epsilon = 0.001\n",
    "sig_s = 0.5\n",
    "lb = 1.\n",
    "k = 0.001\n",
    "\n",
    "\n",
    "def phi_tf(t, X, Y, Z):\n",
    "    \"\"\"\n",
    "    Generator of the BSDE\n",
    "\n",
    "    :param t: M x 1\n",
    "    :param X: M x d\n",
    "    :param Y: M x 1\n",
    "    :param Z: M x d\n",
    "    :return: Generator, M x 1\n",
    "    \"\"\"\n",
    "    q = X[:, 1:2]  # M x 1\n",
    "    s = X[:, 0:1]  # M x 1\n",
    "\n",
    "    A = -1 / (eps * q)  # M x 1\n",
    "    neg_partial_q = A * Z[:, 1:2]  # M x 1\n",
    "\n",
    "    return -1 / (4 * self.k) * (s + neg_partial_q) ** 2\n",
    "\n",
    "def g_tf(X):\n",
    "    \"\"\"\n",
    "    Final condition\n",
    "\n",
    "    :param X: M x d\n",
    "    :return: Final condition, M x 1\n",
    "    \"\"\"\n",
    "    q = X[:, 1:2]  # M x 1\n",
    "    s = X[:, 0:1]  # M x 1\n",
    "    val = q * s - lb * (q ** 2)  # M x 1\n",
    "    return val\n",
    "\n",
    "def mu_tf(t, X, Y, Z):\n",
    "    \"\"\"\n",
    "    Drift of the Forward SDE\n",
    "\n",
    "    :param t: M x 1\n",
    "    :param X: M x d\n",
    "    :param Y: M x 1\n",
    "    :param Z: M x d\n",
    "    :return: Drift, M x d\n",
    "    \"\"\"\n",
    "    return tf.zeros(shape=X.shape, dtype='float32')\n",
    "\n",
    "def sigma_tf(t, X, Y):\n",
    "    \"\"\"\n",
    "    Volatility of the Forward SDE\n",
    "\n",
    "    :param t: M x 1\n",
    "    :param X: M x d\n",
    "    :param Y: M x 1\n",
    "    :return: Vol, M x d x d\n",
    "    \"\"\"\n",
    "    val1 = tf.repeat(tf.constant([sig_s, 0], dtype='float32')[None, :],\n",
    "                     X.shape[0],\n",
    "                     axis=0)  # M x 2\n",
    "    val2 = tf.matmul(X, tf.constant([[0, 0], [0, epsilon*1]], dtype='float32'))  # M x 2\n",
    "\n",
    "    return tf.stack((val1, val2), axis=1)  # M x 2 x 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dt = np.zeros((M, N + 1, 1))  # M x (N+1) x 1\n",
    "DW = np.zeros((M, N + 1, d))  # M x (N+1) x D\n",
    "\n",
    "dt = T / N\n",
    "\n",
    "Dt[:, 1:, :] = dt\n",
    "DW[:, 1:, :] = np.sqrt(dt) * np.random.normal(size=(M, N, d))\n",
    "\n",
    "t = np.cumsum(Dt, axis=1)  # M x (N+1) x 1\n",
    "W = np.cumsum(DW, axis=1)  # M x (N+1) x D\n",
    "\n",
    "W = tf.convert_to_tensor(W, dtype='float32')\n",
    "t = tf.convert_to_tensor(t, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = tf.constant([[20, 10000]], dtype='float32')\n",
    "X0 = tf.tile(x0, [M, 1])  # M x D\n",
    "t0 = t[:, 0, :]\n",
    "W0 = W[:, 0, :]\n",
    "\n",
    "for n in range(0, N):\n",
    "    t1 = t[:, n + 1, :]\n",
    "    W1 = W[:, n + 1, :]\n",
    "    X1 = X0 + mu_tf(t0, X0, 0, 0) * (t1 - t0) + tf.squeeze(\n",
    "        tf.matmul(sigma_tf(t0, X0, 0), tf.expand_dims(W1 - W0, -1)), axis=[-1])\n",
    "\n",
    "    t0 = t1\n",
    "    W0 = W1\n",
    "    X0 = X1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_825:0\", shape=(4096, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(X0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# mean of X\n",
    "print(tf.math.reduce_mean(X0, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"reduce_std/Sqrt:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# std of X\n",
    "print(tf.math.reduce_std(X0, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'sub_445:0' shape=(4096, 1) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final condition\n",
    "X0[:, 0:1] * X0[:, 1:2] - X0[:, 1:2]**2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
